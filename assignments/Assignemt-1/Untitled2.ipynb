{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 60)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m60\u001b[0m\n\u001b[0;31m    return inp2\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from string import punctuation as punc\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "stop_word = list(stop_words.ENGLISH_STOP_WORDS)\n",
    "\n",
    "\n",
    "corpus = open('Movies_TV.txt').read()\n",
    "\n",
    "corpus = re.sub(r'Domain.*\\n', '', corpus)\n",
    "rows = corpus.split('\\n')\n",
    "rows.remove(rows[-1])\n",
    "#print(rows)\n",
    "#rows2 = rows\n",
    "inputData, y = [], []\n",
    "for row in rows:\n",
    "    _, label, _, review = row.split('\\t')\n",
    "    inputData.append(review)\n",
    "    y.append(label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(inputData[1])\n",
    "print(\"----------------------\")\n",
    "\n",
    "def normlizing_case(inp):\n",
    "\tinp2 = []\n",
    "\tfor row in inp:\n",
    "\t\trow = row.lower()\n",
    "\t\tinp2.append(row)\n",
    "\treturn inp2\n",
    "\n",
    "def unwantd_spaces(inp):\n",
    "\tinp2 = []\n",
    "\tfor row in inp:\n",
    "\t\trow = re.sub(' +', ' ', row)\n",
    "\t\tinp2.append(row)\n",
    "\treturn inp2\n",
    "\n",
    "unwantd_spaces(inputData)\n",
    "#print(inputData[1])\n",
    "after_data2 = []\n",
    "def r_punc(inp):\n",
    "\tinp2 = []\n",
    "\tfor row in inp:\n",
    "\t\trow = re.sub('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', ' ', row).lower()\n",
    "# \t\ttokens = word_tokenize(row)\n",
    "# \t\trow = [i for i in tokens if not re.search(r'\\d', i)]\n",
    "# \t\trow = [ps.stem(i) for i in tokens] #stemming\n",
    "# \t\trow = [wl.lemmatize(i) for i in tokens] #lemmatization\n",
    "\t\tinp2.append(row)\n",
    "    \n",
    "    return inp2\n",
    "\n",
    "\n",
    "def r_stop_words(inp):\n",
    "\tinp2 = []\n",
    "\tfor row in inp:\n",
    "\t\trow = [i for i in row if not i in stop_word]\n",
    "\t\tinp2.append(row)\n",
    "\treturn inp2\n",
    "\n",
    "# def r_punc(inp):\n",
    "# \tinp2 = []\n",
    "# \tfor row in inp:\n",
    "# \t\trow = [i for i in row if not i in punc]\n",
    "# \t\tinp2.append(row)\n",
    "# \treturn inp2\n",
    "\n",
    "def stemming(inp):\n",
    "\tinp2 = []\n",
    "\tfor row in inp: \n",
    "\t\trow = [ps.stem(i) for i in row]\n",
    "\t\tinp2.append(row)\n",
    "\treturn inp2\n",
    "\n",
    "def lemmitization(inp):\n",
    "\tinp2 = []\n",
    "\tfor row in inp: \n",
    "\t\trow = [wl.lemmatize(i,'v') for i in row]\n",
    "\t\tinp2.append(row)\n",
    "\treturn inp2\n",
    "\n",
    "inputData = unwantd_spaces(inputData)\n",
    "inputData = normlizing_case(inputData)\n",
    "inputData = r_punc(inputData)\n",
    "#inputData = r_stop_words(inputData)\n",
    "#inputData = r_punc(inputData)\n",
    "#inputData = stemming(inputData)\n",
    "#inputData = r_punc(inputData)\n",
    "#inputData = lemmitization(inputData)\n",
    "print(inputData[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 59)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m59\u001b[0m\n\u001b[0;31m    return inp2\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from string import punctuation as punc\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "stop_word = list(stop_words.ENGLISH_STOP_WORDS)\n",
    "\n",
    "\n",
    "corpus = open('Movies_TV.txt').read()\n",
    "\n",
    "corpus = re.sub(r'Domain.*\\n', '', corpus)\n",
    "rows = corpus.split('\\n')\n",
    "rows.remove(rows[-1])\n",
    "#print(rows)\n",
    "#rows2 = rows\n",
    "inputData, y = [], []\n",
    "for row in rows:\n",
    "    _, label, _, review = row.split('\\t')\n",
    "    inputData.append(review)\n",
    "    y.append(label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(inputData[1])\n",
    "print(\"----------------------\")\n",
    "\n",
    "def normlizing_case(inp):\n",
    "\tinp2 = []\n",
    "\tfor row in inp:\n",
    "\t\trow = row.lower()\n",
    "\t\tinp2.append(row)\n",
    "\treturn inp2\n",
    "\n",
    "def unwantd_spaces(inp):\n",
    "\tinp2 = []\n",
    "\tfor row in inp:\n",
    "\t\trow = re.sub(' +', ' ', row)\n",
    "\t\tinp2.append(row)\n",
    "\treturn inp2\n",
    "\n",
    "unwantd_spaces(inputData)\n",
    "#print(inputData[1])\n",
    "after_data2 = []\n",
    "def r_punc(inp):\n",
    "\tinp2 = []\n",
    "\tfor row in inp:\n",
    "\t\trow = re.sub('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', ' ', row).lower()\n",
    "# \t\ttokens = word_tokenize(row)\n",
    "# \t\trow = [i for i in tokens if not re.search(r'\\d', i)]\n",
    "# \t\trow = [ps.stem(i) for i in tokens] #stemming\n",
    "# \t\trow = [wl.lemmatize(i) for i in tokens] #lemmatization\n",
    "\t\tinp2.append(row)\n",
    "    return inp2\n",
    "\n",
    "\n",
    "def r_stop_words(inp):\n",
    "\tinp2 = []\n",
    "\tfor row in inp:\n",
    "\t\trow = [i for i in row if not i in stop_word]\n",
    "\t\tinp2.append(row)\n",
    "\treturn inp2\n",
    "\n",
    "# def r_punc(inp):\n",
    "# \tinp2 = []\n",
    "# \tfor row in inp:\n",
    "# \t\trow = [i for i in row if not i in punc]\n",
    "# \t\tinp2.append(row)\n",
    "# \treturn inp2\n",
    "\n",
    "def stemming(inp):\n",
    "\tinp2 = []\n",
    "\tfor row in inp: \n",
    "\t\trow = [ps.stem(i) for i in row]\n",
    "\t\tinp2.append(row)\n",
    "\treturn inp2\n",
    "\n",
    "def lemmitization(inp):\n",
    "\tinp2 = []\n",
    "\tfor row in inp: \n",
    "\t\trow = [wl.lemmatize(i,'v') for i in row]\n",
    "\t\tinp2.append(row)\n",
    "\treturn inp2\n",
    "\n",
    "inputData = unwantd_spaces(inputData)\n",
    "inputData = normlizing_case(inputData)\n",
    "inputData = r_punc(inputData)\n",
    "#inputData = r_stop_words(inputData)\n",
    "#inputData = r_punc(inputData)\n",
    "#inputData = stemming(inputData)\n",
    "#inputData = r_punc(inputData)\n",
    "#inputData = lemmitization(inputData)\n",
    "print(inputData[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from string import punctuation as punc\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "stop_word = list(stop_words.ENGLISH_STOP_WORDS)\n",
    "\n",
    "\n",
    "corpus = open('Movies_TV.txt').read()\n",
    "\n",
    "corpus = re.sub(r'Domain.*\\n', '', corpus)\n",
    "rows = corpus.split('\\n')\n",
    "rows.remove(rows[-1])\n",
    "inputData, y = [], []\n",
    "for row in rows:\n",
    "    _, label, _, review = row.split('\\t')\n",
    "    inputData.append(review)\n",
    "    y.append(label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(inputData[1])\n",
    "print(\"----------------------\")\n",
    "\n",
    "def normlizing_case(inp):\n",
    "    inp2 = []\n",
    "    for row in inp:\n",
    "        row = row.lower()\n",
    "        inp2.append(row)\n",
    "    return inp2\n",
    "\n",
    "def unwantd_spaces(inp):\n",
    "    inp2 = []\n",
    "    for row in inp:\n",
    "        row = re.sub(' +', ' ', row)\n",
    "        inp2.append(row)\n",
    "    return inp2\n",
    "\n",
    "\n",
    "def r_punc(inp):\n",
    "    inp2 = []\n",
    "    for row in inp:\n",
    "        row = re.sub('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', ' ', row).lower()\n",
    "        inp2.append(row)\n",
    "    return unwantd_spaces(inp2)\n",
    "\n",
    "\n",
    "#### Stop Words #####\n",
    "def r_stop_words(inp):\n",
    "    inp2 = []\n",
    "    for row in inp:\n",
    "        tokens = word_tokenize(row)\n",
    "        row = [i for i in tokens if not i in stop_word]\n",
    "        inp2.append(row)\n",
    "    return inp2\n",
    "\n",
    "\n",
    "\n",
    "def stemming(inp):\n",
    "\tinp2 = []\n",
    "\tfor row in inp: \n",
    "\t\trow = [ps.stem(i) for i in row]\n",
    "\t\tinp2.append(row)\n",
    "\treturn inp2\n",
    "\n",
    "def lemmitization(inp):\n",
    "    inp2 = []\n",
    "    for row in inp:\n",
    "        row = [wl.lemmatize(i) for i in row]\n",
    "        inp2.append(row)\n",
    "    return inp2\n",
    "\n",
    "def r_num(inp):\n",
    "    inp2 = []\n",
    "    for row in inp:\n",
    "        row = [i for i in row if not re.search(r'\\d', i)]\n",
    "        inp2.append(row)\n",
    "    return inp2\n",
    "\n",
    "inputData = unwantd_spaces(inputData)\n",
    "inputData = normlizing_case(inputData)\n",
    "inputData = r_punc(inputData)\n",
    "inputData = r_stop_words(inputData)\n",
    "inputData = stemming(inputData)\n",
    "inputData = lemmitization(inputData)\n",
    "inputData = r_num(inputData)\n",
    "print(inputData[7])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4yr', 'old', 'son', 'love', 'cartoon', 'buy', 'stori', 'intrest']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmitization(inp):\n",
    "    inp2 = []\n",
    "    for row in inp:\n",
    "        row = wl.lemmatize(row)\n",
    "        inp2.append(row)\n",
    "    return inp2\n",
    "a = lemmitization(['4yr', 'old', 'son', 'love', 'cartoon', 'buy', 'stori', 'intrest'])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import \tWordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

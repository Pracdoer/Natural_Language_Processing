{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-dbe8f6dbd11b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpunctuation\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from string import punctuation as punc\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "stop_word = list(stop_words.ENGLISH_STOP_WORDS)\n",
    "\n",
    "\n",
    "corpus = open('Movies_TV.txt').read()\n",
    "\n",
    "corpus = re.sub(r'Domain.*\\n', '', corpus)\n",
    "rows = corpus.split('\\n')\n",
    "rows.remove(rows[-1])\n",
    "inputData, y = [], []\n",
    "for row in rows:\n",
    "    _, label, _, review = row.split('\\t')\n",
    "    inputData.append(review)\n",
    "    y.append(label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(inputData[1])\n",
    "print(\"----------------------\")\n",
    "\n",
    "def normlizing_case(inp):\n",
    "    inp2 = []\n",
    "    for row in inp:\n",
    "        row = row.lower()\n",
    "        inp2.append(row)\n",
    "    return inp2\n",
    "\n",
    "def unwantd_spaces(inp):\n",
    "    inp2 = []\n",
    "    for row in inp:\n",
    "        row = re.sub(' +', ' ', row)\n",
    "        inp2.append(row)\n",
    "    return inp2\n",
    "\n",
    "\n",
    "def r_punc(inp):\n",
    "    inp2 = []\n",
    "    for row in inp:\n",
    "        row = re.sub('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', ' ', row).lower()\n",
    "        inp2.append(row)\n",
    "    return unwantd_spaces(inp2)\n",
    "\n",
    "\n",
    "#### Stop Words #####\n",
    "def r_stop_words(inp):\n",
    "    inp2 = []\n",
    "    for row in inp:\n",
    "        words = word_tokenize(row)\n",
    "        words = [word for word in words if not word in stop_word]\n",
    "        row = ' '.join(words)\n",
    "        inp2.append(row)\n",
    "    return inp2\n",
    "\n",
    "\n",
    "\n",
    "def stemming(inp):\n",
    "    inp2 = []\n",
    "    for row in inp:\n",
    "        words = word_tokenize(row)\n",
    "        words = [ps.stem(word) for word in words if not word in stop_word]\n",
    "        row = ' '.join(words)\n",
    "        inp2.append(row)\n",
    "    return inp2\n",
    "\n",
    "def lemmitization(inp):\n",
    "    inp2 = []\n",
    "    for row in inp:\n",
    "        words = word_tokenize(row)\n",
    "        words = [wl.lemmatize(word) for word in words if not word in stop_word]\n",
    "        row = ' '.join(words)\n",
    "        inp2.append(row)\n",
    "    return inp2\n",
    "\n",
    "def r_num(inp):\n",
    "    inp2 = []\n",
    "    for row in inp:\n",
    "        row = [i for i in row if not re.search(r'\\d', i)]\n",
    "        inp2.append(row)\n",
    "    return inp2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-55047e974068>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#words = words[:100]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0munigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "#words = words[:100]\n",
    "sentence = inputData[0]\n",
    "words = sentence.split(' ')\n",
    "def Ngrams(words)\n",
    "    unigrams = list(ngrams(words, 1))\n",
    "    bigrams = list(ngrams(words, 2))\n",
    "    trigrams = list(ngrams(words, 3))\n",
    "    print(\"unigrams\", unigrams)\n",
    "    print(\"---------\")\n",
    "    print(\"bigrams\", bigrams)\n",
    "    print(\"---------\")\n",
    "    print(\"trigrams\", trigrams)\n",
    "\n",
    "def NgramsProb(words):\n",
    "    unigrams_prob = [words.count(x)/len(set(words)) for x in words]\n",
    "    print(\"unigram Probs\\n\")\n",
    "    print(unigrams_prob)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"\\nbi grams Probs\\n\")\n",
    "    bigrams_freq = []\n",
    "    for b in bigrams:\n",
    "        temp = bigrams.count(b)\n",
    "        temp2 = words.count(b[0])\n",
    "        bigrams_freq.append(temp/temp2)\n",
    "    print(bigrams_freq)\n",
    "    \n",
    "    print(\"\\nbi grams Probs\\n\")\n",
    "    trigrams_freq = [trigrams.count(x)/bigrams.count(x[:2]) for x in trigrams]\n",
    "    print(trigrams_freq)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_freq = [words.count(x)/len(set(words)) for x in words]\n",
    "unigrams_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_freq = []\n",
    "for b in bigrams:\n",
    "    temp = bigrams.count(b)\n",
    "    temp2 = words.count(b[0])\n",
    "    bigrams_freq.append(temp/temp2)\n",
    "bigrams_freq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totel_tokens(inp):\n",
    "    inp2 = []\n",
    "    for row in inp:\n",
    "        words = word_tokenize(row)\n",
    "        inp2.append(words)\n",
    "    return len(inp2)\n",
    "t_tokens = totel_tokens(inputData)\n",
    "print(\"total tokens: \", t_tokens)\n",
    "print(\"Unique tokens: \" set(t_tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_reivews(inp):\n",
    "    avg_len = []\n",
    "    for row in inp:\n",
    "        rl = rl + len(row)\n",
    "        avg_len.append(float(rl/len(inp)))\n",
    "    return avg_len\n",
    "print(\"Average length of reivews: \", avg_len(inputData))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_token(inp):\n",
    "    avg_tokens = []\n",
    "    for row in inp:\n",
    "        rl = len(row)\n",
    "        words = word_tokenize(row)\n",
    "        tl = float(len(words)/rl)\n",
    "        avg_tokens.append(tl)\n",
    "    return avg_tokens\n",
    "print(\"average length of tokens with in a reivews: \", avg_token(inputData))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Section\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from string import punctuation as punc\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "stop_word = list(stop_words.ENGLISH_STOP_WORDS)\n",
    "\n",
    "\n",
    "def Display():\n",
    "    print(\"Select below\\n\")\n",
    "    print(\"------------\\n\")\n",
    "    print(\"1: Text Normilization Process\\n\")\n",
    "    print(\"2: Ngrams Sections\\n\")\n",
    "    print(\"3 Average Sections\\n\")\n",
    "    inp = input(\"Enter your value\\n\")\n",
    "    inp = int(inp)\n",
    "    \n",
    "    if(inp == 1):\n",
    "        print(\" Removing unwanted whitespaces \\n\", inputData = unwantd_spaces(inputData))\n",
    "        print(\" Normalizing case\\n \", inputData = normlizing_case(inputData))\n",
    "        print(\"Removing Stop_words\\n \", inputData = r_stop_words(inputData))\n",
    "        print(\"Removing punctuations\\n \", inputData = r_punc(inputData))\n",
    "        print(\"Stemming words process\\n \", inputData = stemming(inputData))\n",
    "        print(\"Lemmatizing words\\n \", inputData = lemmitization(inputData))\n",
    "        print(\"Data after Processing\\n\\n\")\n",
    "        print(inputData)\n",
    "        return \n",
    "    if(inp == 2):\n",
    "        print(\"Ngrams:\\n\", Ngrams(words))\n",
    "        print(\"Ngrams Probs\\n \", NgramsProb(words))\n",
    "        return\n",
    "    if(inp == 3):\n",
    "        totel_tokens(inputData)\n",
    "        print(\"\\n\")\n",
    "        avg_reivews(inputData)\n",
    "        print(\"\\n\")\n",
    "        avg_token(inputData)\n",
    "        print(\"\\n\")\n",
    "        return\n",
    "    else:\n",
    "        print(\"You entered wrong number\\n\")\n",
    "        return Display()\n",
    "        \n",
    "        \n",
    "corpus = open('Movies_TV.txt').read()\n",
    "corpus = re.sub(r'Domain.*\\n', '', corpus)\n",
    "rows = corpus.split('\\n')\n",
    "rows.remove(rows[-1])\n",
    "inputData, y = [], []\n",
    "for row in rows:\n",
    "    _, label, _, review = row.split('\\t')\n",
    "    inputData.append(review)\n",
    "    y.append(label)\n",
    "\n",
    "diplay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'inputData' is an invalid keyword argument for this function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-32f38d43eb1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minputData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minputData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m34\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m34\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"afadsf \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'inputData' is an invalid keyword argument for this function"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
